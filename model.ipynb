{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/temp_id/anaconda3/envs/py311_sh/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch.fft\n",
    "import math\n",
    "import numpy as np\n",
    "from mamba_ssm import Mamba\n",
    "from einops import rearrange, repeat, einsum\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from simba import Block_mamba, ClassBlock\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "import arff\n",
    "\n",
    "'''\n",
    "Simple SiMBA model\n",
    "x -> embedding layer -> SiMBA blocks -> output layer -> output\n",
    "'''\n",
    "class SiMBASimple(nn.Module):\n",
    "    def __init__(self,\n",
    "        num_classes, \n",
    "        input_dim, \n",
    "        embed_dim=64, \n",
    "        mlp_ratio=4, \n",
    "        num_blocks=4,\n",
    "        scale=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block_mamba(dim=embed_dim, mlp_ratio=mlp_ratio, cm_type=\"EinFFT\")\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x): # x : B x L x D\n",
    "        x = self.embedding(x)\n",
    "        B, L, D = x.shape\n",
    "        for block in self.blocks:\n",
    "            x = block(x, L, D)\n",
    "        x = self.norm(x.mean(dim=1))\n",
    "        x = self.head(x) * self.scale\n",
    "        return x\n",
    "\n",
    "'''\n",
    "Model based on SiMBA image classification model architecture \n",
    "https://github.com/badripatro/simba\n",
    "'''\n",
    "class SiMBA(nn.Module):\n",
    "    def __init__(self,\n",
    "        input_dim,\n",
    "        num_classes,\n",
    "        embed_dims=[64, 128, 320, 448],\n",
    "        mlp_ratios=[8, 8, 4, 4], \n",
    "        drop_path_rate=0., \n",
    "        norm_layer=nn.LayerNorm,\n",
    "        depths=[3, 4, 6, 3], \n",
    "        sr_ratios=[4, 2, 1, 1], \n",
    "        num_stages=4,\n",
    "        scale=1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.num_classes = num_classes\n",
    "        self.depths = depths\n",
    "        self.num_stages = num_stages\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        cur = 0\n",
    "        alpha=5\n",
    "        for i in range(num_stages):\n",
    "            if i == 0:\n",
    "                patch_embed = nn.Linear(input_dim, embed_dims[i])\n",
    "            else:\n",
    "                patch_embed = nn.Linear(embed_dims[i - 1], embed_dims[i])\n",
    "\n",
    "            block = nn.ModuleList([Block_mamba(\n",
    "                dim=embed_dims[i], \n",
    "                mlp_ratio = mlp_ratios[i], \n",
    "                drop_path=dpr[cur + j], \n",
    "                norm_layer=norm_layer,\n",
    "                sr_ratio = sr_ratios[i],\n",
    "                cm_type='EinFFT')\n",
    "            for j in range(depths[i])])\n",
    "\n",
    "            norm = norm_layer(embed_dims[i])\n",
    "            cur += depths[i]\n",
    "\n",
    "            setattr(self, f\"patch_embed{i + 1}\", patch_embed)\n",
    "            setattr(self, f\"block{i + 1}\", block)\n",
    "            setattr(self, f\"norm{i + 1}\", norm)\n",
    "\n",
    "        post_layers = ['ca']\n",
    "        self.post_network = nn.ModuleList([\n",
    "            ClassBlock(\n",
    "                dim = embed_dims[-1], \n",
    "                mlp_ratio = mlp_ratios[-1],\n",
    "                norm_layer=norm_layer,\n",
    "                cm_type='EinFFT')\n",
    "            for _ in range(len(post_layers))\n",
    "        ])\n",
    "        self.head = nn.Linear(embed_dims[-1], num_classes)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward_cls(self, x, H, W):\n",
    "        B, N, C = x.shape\n",
    "        cls_tokens = x.mean(dim=1, keepdim=True)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        for block in self.post_network:\n",
    "            x = block(x, H, W)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, D = x.shape\n",
    "        for i in range(self.num_stages):\n",
    "            patch_embed = getattr(self, f\"patch_embed{i + 1}\")\n",
    "            block = getattr(self, f\"block{i + 1}\")\n",
    "            x = patch_embed(x)\n",
    "            for blk in block:\n",
    "                x = blk(x, L, D)\n",
    "            \n",
    "            if i != self.num_stages - 1:\n",
    "                norm = getattr(self, f\"norm{i + 1}\")\n",
    "                x = norm(x)\n",
    "                #x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        x = self.forward_cls(x, L, D)[:, 0]\n",
    "        norm = getattr(self, f\"norm{self.num_stages}\")\n",
    "        x = norm(x)\n",
    "        x = self.head(x) * self.scale\n",
    "        return x\n",
    "\n",
    "# train code\n",
    "\n",
    "# 1. train codes for regression model\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze()\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "            #print(outputs, targets)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, learning_rate, device='cuda'):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss = validate_one_epoch(model, val_loader, criterion, device)\n",
    "        print(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print('Model saved!')\n",
    "    print('Training complete.')\n",
    "\n",
    "# 2. train codes for classification model\n",
    "\n",
    "def train_one_epoch_c(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, targets in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return epoch_loss, accuracy\n",
    "\n",
    "def validate_one_epoch_c(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "            \n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return epoch_loss, accuracy\n",
    "\n",
    "def train_model_c(model, train_loader, val_loader, num_epochs, learning_rate, device='cuda'):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        train_loss, train_acc = train_one_epoch_c(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate_one_epoch_c(model, val_loader, criterion, device)\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print('Model saved!')\n",
    "    print('Training complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification - handwriting dataset\n",
    "# https://www.timeseriesclassification.com/description.php?Dataset=Handwriting\n",
    "\n",
    "def load_arff_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        dataset = arff.load(f)\n",
    "    data = np.array(dataset['data'])\n",
    "    X = data[:, :-1].astype(np.float32)\n",
    "    y = data[:, -1].astype(np.float32)\n",
    "    y = y.astype(np.int64) - 1\n",
    "    return X, y\n",
    "\n",
    "class HandwritingDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def load_handwriting(data_dir='./dataset/handwriting', batch=32):\n",
    "    X1_test, y_test = load_arff_data(f'{data_dir}/HandwritingDimension1_TRAIN.arff')\n",
    "    X2_test, _ = load_arff_data(f'{data_dir}/HandwritingDimension2_TRAIN.arff')\n",
    "    X3_test, _ = load_arff_data(f'{data_dir}/HandwritingDimension3_TRAIN.arff')\n",
    "    X1_train, y_train = load_arff_data(f'{data_dir}/HandwritingDimension1_TEST.arff')\n",
    "    X2_train, _ = load_arff_data(f'{data_dir}/HandwritingDimension2_TEST.arff')\n",
    "    X3_train, _ = load_arff_data(f'{data_dir}/HandwritingDimension3_TEST.arff')\n",
    "    X_train = np.stack([X1_train, X2_train, X3_train], axis=-1)\n",
    "    X_test = np.stack([X1_test, X2_test, X3_test], axis=-1)\n",
    "\n",
    "    train_dataset = HandwritingDataset(X_train, y_train)\n",
    "    test_dataset = HandwritingDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def run_handwriting(batch=32, epochs=12, lr=3e-4, simple=False):\n",
    "    train_loader, test_loader = load_handwriting(batch=batch)\n",
    "    model = SiMBASimple(input_dim=3, num_classes=26) if simple else SiMBA(input_dim=3, num_classes=26)\n",
    "    train_model_c(model, train_loader, test_loader, num_epochs=epochs, learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regressoin - TAC\n",
    "\n",
    "class TACDataset(Dataset):\n",
    "    def __init__(self, data_dir, sequence_length=1000):\n",
    "        self.data_dir = data_dir\n",
    "        self.sequence_length = sequence_length\n",
    "        self.data = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        data_list = []\n",
    "        pids = [f.split(' ')[0] for f in os.listdir(self.data_dir) if 'CAM Results.csv' in f]\n",
    "        \n",
    "        for pid in pids:\n",
    "            cam_file = os.path.join(self.data_dir, f'{pid} CAM Results.csv')\n",
    "            cam_data = pd.read_csv(cam_file)\n",
    "            cam_data['Time'] = pd.to_datetime(cam_data['Time'])\n",
    "            acc_file = os.path.join(self.data_dir, f'{pid}_acc_data.csv')\n",
    "            acc_data = pd.read_csv(acc_file)\n",
    "            acc_data['time'] = pd.to_datetime(acc_data['time'])\n",
    "            acc_data = acc_data.sort_values('time').reset_index(drop=True)\n",
    "            \n",
    "            for idx, row in cam_data.iterrows():\n",
    "                tac_time = row['Time']\n",
    "                tac_value = row['TAC Level']\n",
    "                \n",
    "                acc_subset = acc_data[acc_data['time'] < tac_time].tail(self.sequence_length)\n",
    "                if len(acc_subset) == self.sequence_length:\n",
    "                    x = acc_subset[['x', 'y', 'z']].values\n",
    "                    y = tac_value\n",
    "                    data_list.append((x, y))\n",
    "        return data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.data[idx]\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "def load_tac(data_dir='./dataset/tac', batch=32, seq_len=1000):\n",
    "    dataset = TACDataset(data_dir=data_dir, sequence_length=seq_len)\n",
    "    train_idx, val_idx = train_test_split(np.arange(len(dataset)), test_size=0.2, random_state=42)\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "\n",
    "    train_loader = DataLoader(dataset, batch_size=batch, sampler=train_sampler)\n",
    "    val_loader = DataLoader(dataset, batch_size=batch, sampler=val_sampler)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def run_tac(seq_len=1000, batch=32, epochs=12, lr=3e-3, simple=False):\n",
    "    train_loader, val_loader = load_tac(batch=batch, seq_len=seq_len)\n",
    "    model = SiMBASimple(input_dim=3, num_classes=1) if simple else SiMBA(input_dim=3, num_classes=1)\n",
    "    train_model(model, train_loader, val_loader, num_epochs=epochs, learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression - Lamp\n",
    "\n",
    "class LAMPDataset(Dataset):\n",
    "    def __init__(self, data_dir, y_path, seq_len, target, is_regression):\n",
    "        self.data_dir = data_dir\n",
    "        self.seq_len = seq_len\n",
    "        self.y_data = pd.read_csv(y_path)\n",
    "        self.uuids = self.y_data['uuid'].values\n",
    "        self.scores = self.y_data[target].values\n",
    "        self.scaler = StandardScaler()\n",
    "        self.is_regression = is_regression\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.uuids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        uuid = self.uuids[idx]\n",
    "        score = self.scores[idx]\n",
    "        file_path = os.path.join(self.data_dir, f'{uuid}.csv')\n",
    "        data = pd.read_csv(file_path)\n",
    "        data = data.sort_values(by='timestamp').reset_index(drop=True)\n",
    "        time_series_data = data.drop(columns=['timestamp']).values\n",
    "        time_series_data = self.scaler.fit_transform(time_series_data)\n",
    "        if time_series_data.shape[0] < self.seq_len:\n",
    "            padding = np.zeros((self.seq_len - time_series_data.shape[0], time_series_data.shape[1]))\n",
    "            time_series_data = np.vstack((time_series_data, padding))\n",
    "        elif time_series_data.shape[0] > self.seq_len:\n",
    "            time_series_data = time_series_data[:self.seq_len, :]\n",
    "        if self.is_regression:\n",
    "            return torch.tensor(time_series_data, dtype=torch.float32), torch.tensor(score, dtype=torch.float32)\n",
    "        else:\n",
    "            return torch.tensor(time_series_data, dtype=torch.float32), torch.tensor(score, dtype=torch.int64)\n",
    "\n",
    "def load_lamp(target, batch_size, seq_len, is_regression, data_dir='./dataset/lamp', y_file='./dataset/lamp/student_info.csv'):\n",
    "    dataset = LAMPDataset(data_dir=data_dir, y_path=y_file, seq_len=seq_len, target=target, is_regression=is_regression)\n",
    "    train_idx, val_idx = train_test_split(np.arange(len(dataset)), test_size=0.2, random_state=42)\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def run_regression(target, batch=8, seq_len=6000, epochs=12, lr=3e-3, simple=False):\n",
    "    train_loader, val_loader = load_lamp(target=target, batch_size=batch, seq_len=seq_len, is_regression=True)\n",
    "    model = SiMBASimple(input_dim=15, num_classes=1, scale=80) if simple else SiMBA(input_dim=15, num_classes=1, scale=1)\n",
    "    train_model(model, train_loader, val_loader, num_epochs=epochs, learning_rate=lr)\n",
    "\n",
    "def run_classification(target, batch=8, seq_len=6000, epochs=12, lr=3e-3, simple=False):\n",
    "    train_loader, val_loader = load_lamp(target=target, batch_size=batch, seq_len=seq_len, is_regression=False)\n",
    "    model = SiMBASimple(input_dim=15, num_classes=2, scale=1) if simple else SiMBA(input_dim=15, num_classes=2, scale=1)\n",
    "    train_model_c(model, train_loader, val_loader, num_epochs=epochs, learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_handwriting()\n",
    "run_tac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 7/8 [00:03<00:00,  1.91it/s]/home/temp_id/anaconda3/envs/py311_sh/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 600.5672, Val Loss: 37.8623\n",
      "Model saved!\n",
      "Epoch 2/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 7/8 [00:03<00:00,  1.88it/s]/home/temp_id/anaconda3/envs/py311_sh/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 124.6956, Val Loss: 45.5317\n",
      "Epoch 3/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 7/8 [00:03<00:00,  1.77it/s]/home/temp_id/anaconda3/envs/py311_sh/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 118.2643, Val Loss: 34.6985\n",
      "Model saved!\n",
      "Epoch 4/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 7/8 [00:03<00:00,  1.88it/s]/home/temp_id/anaconda3/envs/py311_sh/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 103.2348, Val Loss: 35.7588\n",
      "Epoch 5/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 7/8 [00:03<00:00,  1.88it/s]/home/temp_id/anaconda3/envs/py311_sh/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 125.4829, Val Loss: 37.9688\n",
      "Epoch 6/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 7/8 [00:03<00:00,  2.10it/s]/home/temp_id/anaconda3/envs/py311_sh/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 109.0614, Val Loss: 35.3274\n",
      "Epoch 7/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 7/8 [00:03<00:00,  1.98it/s]/home/temp_id/anaconda3/envs/py311_sh/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 116.5639, Val Loss: 38.6360\n",
      "Epoch 8/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 7/8 [00:03<00:00,  1.90it/s]/home/temp_id/anaconda3/envs/py311_sh/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 96.2602, Val Loss: 38.8472\n",
      "Epoch 9/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 7/8 [00:03<00:00,  1.93it/s]/home/temp_id/anaconda3/envs/py311_sh/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 92.9007, Val Loss: 35.8911\n",
      "Epoch 10/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 7/8 [00:03<00:00,  1.99it/s]/home/temp_id/anaconda3/envs/py311_sh/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 93.5012, Val Loss: 38.2805\n",
      "Epoch 11/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 7/8 [00:03<00:00,  2.00it/s]/home/temp_id/anaconda3/envs/py311_sh/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 99.4943, Val Loss: 36.1101\n",
      "Epoch 12/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 7/8 [00:03<00:00,  2.01it/s]/home/temp_id/anaconda3/envs/py311_sh/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 84.6787, Val Loss: 39.6269\n",
      "Training complete.\n",
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6348, Train Acc: 0.5088, Val Loss: 0.1664, Val Acc: 0.3333\n",
      "Model saved!\n",
      "Epoch 2/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5567, Train Acc: 0.4737, Val Loss: 0.1308, Val Acc: 0.6667\n",
      "Model saved!\n",
      "Epoch 3/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5148, Train Acc: 0.6316, Val Loss: 0.1344, Val Acc: 0.6667\n",
      "Epoch 4/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5189, Train Acc: 0.6316, Val Loss: 0.1299, Val Acc: 0.6667\n",
      "Model saved!\n",
      "Epoch 5/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5182, Train Acc: 0.6316, Val Loss: 0.1313, Val Acc: 0.6667\n",
      "Epoch 6/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5294, Train Acc: 0.5789, Val Loss: 0.1410, Val Acc: 0.6667\n",
      "Epoch 7/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5031, Train Acc: 0.6491, Val Loss: 0.1288, Val Acc: 0.6667\n",
      "Model saved!\n",
      "Epoch 8/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5237, Train Acc: 0.6316, Val Loss: 0.1382, Val Acc: 0.6667\n",
      "Epoch 9/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5353, Train Acc: 0.5614, Val Loss: 0.1294, Val Acc: 0.6667\n",
      "Epoch 10/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5323, Train Acc: 0.6316, Val Loss: 0.1361, Val Acc: 0.6667\n",
      "Epoch 11/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5546, Train Acc: 0.5614, Val Loss: 0.1384, Val Acc: 0.6667\n",
      "Epoch 12/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5228, Train Acc: 0.6316, Val Loss: 0.1330, Val Acc: 0.6667\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "run_regression(target='STAI-X-1', simple=True)\n",
    "run_classification(target=\"is_STAI-X-1\", simple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple test code\n",
    "\n",
    "def load_single_test_data(file_path, seq_len=1440):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data = data.sort_values(by='timestamp').reset_index(drop=True)\n",
    "    time_series_data = data.drop(columns=['timestamp']).values\n",
    "    scaler = StandardScaler()\n",
    "    time_series_data = scaler.fit_transform(time_series_data)\n",
    "\n",
    "    if time_series_data.shape[0] < seq_len:\n",
    "        padding = np.zeros((seq_len - time_series_data.shape[0], time_series_data.shape[1]))\n",
    "        time_series_data = np.vstack((time_series_data, padding))\n",
    "    elif time_series_data.shape[0] > seq_len:\n",
    "        time_series_data = time_series_data[:seq_len, :]\n",
    "\n",
    "    return torch.tensor(time_series_data, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "def run_inference(model, test_data, device='cuda'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_data = test_data.to(device)\n",
    "        output = model(test_data)\n",
    "    \n",
    "    return output.item()\n",
    "\n",
    "def test(uuid):\n",
    "    test_file_path = f'./dataset/lamp/{uuid}.csv'\n",
    "    model_path = 'best_model.pth'\n",
    "\n",
    "    model = SiMBA(input_dim=15, num_classes=1, scale=1)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    test_data = load_single_test_data(test_file_path, seq_len = 6000)\n",
    "\n",
    "    prediction = run_inference(model, test_data)\n",
    "    print(f\"Predicted score: {prediction}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_sh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
