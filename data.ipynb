{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sensor_data(base_dir, sensor):\n",
    "    total_df = pd.DataFrame()\n",
    "    for week in range(4, 5):\n",
    "        week_dir = os.path.join(uuid_path, f'week{week}/lamp.{sensor}')\n",
    "        if os.path.exists(week_dir):\n",
    "            for csv_file in glob.glob(os.path.join(week_dir, '*.csv')):\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_file)\n",
    "                    if 'timestamp' not in df.columns:\n",
    "                        continue\n",
    "                    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "                    df['timestamp'] = df['timestamp'] + pd.DateOffset(hours=9)\n",
    "                    df = df.sort_values(by='timestamp').reset_index(drop=True)\n",
    "                    df = df.set_index('timestamp')\n",
    "                    df = df.resample('1T').first()\n",
    "                    df = df.dropna()\n",
    "                    total_df = pd.concat([total_df, df])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file {week_dir}/{csv_file}: {e}\")\n",
    "                    continue\n",
    "    return total_df\n",
    "\n",
    "base_dir = '/data_248/mindlamp/raw_data/'\n",
    "sensors = ['device_motion']\n",
    "uuids = pd.read_csv('./dataset/student_info.csv')['uuid'].tolist()\n",
    "for session in range(1, 5):\n",
    "    session_dir = os.path.join(base_dir, f'session{session}')\n",
    "    for uuid_dir in os.listdir(session_dir):\n",
    "        if uuid_dir not in uuids:\n",
    "            print(f\"{uuid_dir} not in mapping\")\n",
    "            continue\n",
    "        uuid_path = os.path.join(session_dir, uuid_dir)\n",
    "        df = get_sensor_data(base_dir, 'device_motion')\n",
    "        df.to_csv(f'dataset/{uuid_dir}.csv')\n",
    "        print(f\"save {uuid_dir} data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = './dataset/20201015_consumption.xlsx'\n",
    "file2 = './dataset/20201015_profiles.xlsx'\n",
    "file3 = './dataset/20201015_weather.xlsx'\n",
    "df1 = pd.read_excel(file1)\n",
    "df2 = pd.read_excel(file2)\n",
    "df3 = pd.read_excel(file3)\n",
    "df1.to_csv('consumptions.csv')\n",
    "df2.to_csv('profiles.csv')\n",
    "df3.to_csv('weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'activity'], dtype='object')\n",
      "Index(['date', '5d6fcd1cf44b0324bc6b7254', '5d6fcd1cf44b0324bc6b7257',\n",
      "       '5d6fcd1cf44b0324bc6b725a', '5d6fcd1cf44b0324bc6b725d',\n",
      "       '5d6fcd1df44b0324bc6b7260', '5d6fcd1df44b0324bc6b726b',\n",
      "       '5d6fcd1df44b0324bc6b7271', '5d6fcd1df44b0324bc6b7274',\n",
      "       '5d6fcd1ef44b0324bc6b727a',\n",
      "       ...\n",
      "       '5d6fcd7cf44b0324bc6b79e4', '5d6fcd7df44b0324bc6b79ed',\n",
      "       '5d6fcd7df44b0324bc6b79ef', '5d6fcd7df44b0324bc6b79f1',\n",
      "       '5d6fcd7df44b0324bc6b79f4', '5d6fcd7df44b0324bc6b79f7',\n",
      "       '5d6fcd7df44b0324bc6b79fd', '5d6fcd7ef44b0324bc6b7a00',\n",
      "       '5d6fcd7ef44b0324bc6b7a02', '5d6fcd7ef44b0324bc6b7a03'],\n",
      "      dtype='object', length=500)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "weather_df = pd.read_csv('./dataset/weather.csv').drop(columns=['Unnamed: 0', 'date'])\n",
    "consumptions_df = pd.read_csv('./dataset/consumptions.csv').drop(columns=['Unnamed: 0', 'date'])\n",
    "profiles_df = pd.read_csv('./dataset/profiles.csv').drop(columns=['Unnamed: 0.1'])\n",
    "weather_df.drop\n",
    "\n",
    "print(profiles_df.columns)\n",
    "print(weather_df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data separated and saved to individual CSV files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/bar_crawl/all_accelerometer_data_pids_13.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert time column to datetime format\n",
    "df['time'] = pd.to_datetime(df['time'], unit='ms')\n",
    "\n",
    "# Get the unique pids\n",
    "unique_pids = df['pid'].unique()\n",
    "\n",
    "# Separate data for each pid and save to individual CSV files\n",
    "for pid in unique_pids:\n",
    "    df_pid = df[df['pid'] == pid].drop(columns=['pid'])\n",
    "    output_file = f'./data/{pid}_acc_data.csv'\n",
    "    df_pid.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Data separated and saved to individual CSV files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted CC6740 CAM Results.xlsx to CC6740 CAM Results.csv\n",
      "Converted SF3079 CAM Results.xlsx to SF3079 CAM Results.csv\n",
      "Converted DC6359 CAM Results.xlsx to DC6359 CAM Results.csv\n",
      "Converted BK7610 CAM Results.xlsx to BK7610 CAM Results.csv\n",
      "Converted MJ8002 CAM Results.xlsx to MJ8002 CAM Results.csv\n",
      "Converted DK3500 CAM Results.xlsx to DK3500 CAM Results.csv\n",
      "Converted HV0618 CAM Results.xlsx to HV0618 CAM Results.csv\n",
      "Converted PC6771 CAM Results.xlsx to PC6771 CAM Results.csv\n",
      "Converted JR8022 CAM results.xlsx to JR8022 CAM results.csv\n",
      "Converted BU4707 CAM results.xlsx to BU4707 CAM results.csv\n",
      "Converted MC7070 CAM Results.xlsx to MC7070 CAM Results.csv\n",
      "Converted JB3156 CAM Results.xlsx to JB3156 CAM Results.csv\n",
      "Converted SA0297 CAM Results.xlsx to SA0297 CAM Results.csv\n",
      "All files have been converted.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the directory containing the xlsx files\n",
    "input_dir = './data/bar_crawl/raw_tac'\n",
    "output_dir = './data/bar_crawl/raw_tac'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Loop through all the files in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.xlsx'):\n",
    "        # Construct full file path\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        \n",
    "        # Load the xlsx file\n",
    "        df = pd.read_excel(file_path)\n",
    "        \n",
    "        # Define the output CSV file path\n",
    "        csv_filename = filename.replace('.xlsx', '.csv')\n",
    "        csv_output_path = os.path.join(output_dir, csv_filename)\n",
    "        \n",
    "        # Save the dataframe to a CSV file\n",
    "        df.to_csv(csv_output_path, index=False)\n",
    "        \n",
    "        print(f\"Converted {filename} to {csv_filename}\")\n",
    "\n",
    "print(\"All files have been converted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered and saved SA0297 CAM Results.csv\n",
      "Filtered and saved MC7070 CAM Results.csv\n",
      "Filtered and saved JR8022 CAM results.csv\n",
      "Filtered and saved MJ8002 CAM Results.csv\n",
      "Filtered and saved JB3156 CAM Results.csv\n",
      "Filtered and saved BU4707 CAM results.csv\n",
      "Filtered and saved SF3079 CAM Results.csv\n",
      "Filtered and saved DK3500 CAM Results.csv\n",
      "Filtered and saved PC6771 CAM Results.csv\n",
      "Filtered and saved DC6359 CAM Results.csv\n",
      "Filtered and saved HV0618 CAM Results.csv\n",
      "Filtered and saved CC6740 CAM Results.csv\n",
      "Filtered and saved BK7610 CAM Results.csv\n",
      "All files have been processed.\n"
     ]
    }
   ],
   "source": [
    "input_dir = './data/bar_crawl/raw_tac'\n",
    "output_dir = './data'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# List of columns to keep\n",
    "columns_to_keep = ['TAC Level', 'IR Voltage', 'Temperature', 'Time']\n",
    "\n",
    "# Loop through all the files in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Construct full file path\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        \n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check if the required columns are present in the dataframe\n",
    "        if all(column in df.columns for column in columns_to_keep):\n",
    "            # Keep only the required columns\n",
    "            df_filtered = df[columns_to_keep]\n",
    "            \n",
    "            # Define the output CSV file path\n",
    "            filtered_output_path = os.path.join(output_dir, filename)\n",
    "            \n",
    "            # Save the filtered dataframe to a CSV file\n",
    "            df_filtered.to_csv(filtered_output_path, index=False)\n",
    "            \n",
    "            print(f\"Filtered and saved {filename}\")\n",
    "        else:\n",
    "            print(f\"Skipping {filename}: Not all required columns are present\")\n",
    "\n",
    "print(\"All files have been processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1000, 3])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class TACDataset(Dataset):\n",
    "    def __init__(self, data_dir, sequence_length=1000):\n",
    "        self.data_dir = data_dir\n",
    "        self.sequence_length = sequence_length\n",
    "        self.data = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        # Load and preprocess the data from all the files\n",
    "        data_list = []\n",
    "        pids = [f.split(' ')[0] for f in os.listdir(self.data_dir) if 'CAM Results.csv' in f]\n",
    "        \n",
    "        for pid in pids:\n",
    "            # Load CAM Results data\n",
    "            cam_file = os.path.join(self.data_dir, f'{pid} CAM Results.csv')\n",
    "            cam_data = pd.read_csv(cam_file)\n",
    "            cam_data['Time'] = pd.to_datetime(cam_data['Time'])\n",
    "            \n",
    "            # Load accelerometer data\n",
    "            acc_file = os.path.join(self.data_dir, f'{pid}_acc_data.csv')\n",
    "            acc_data = pd.read_csv(acc_file)\n",
    "            acc_data['time'] = pd.to_datetime(acc_data['time'])\n",
    "            \n",
    "            # Ensure acc_data is sorted by time\n",
    "            acc_data = acc_data.sort_values('time').reset_index(drop=True)\n",
    "            \n",
    "            for idx, row in cam_data.iterrows():\n",
    "                tac_time = row['Time']\n",
    "                tac_value = row['TAC Level']\n",
    "                \n",
    "                # Get the 1000 accelerometer points before the TAC measurement time\n",
    "                acc_subset = acc_data[acc_data['time'] < tac_time].tail(self.sequence_length)\n",
    "                \n",
    "                # Only consider this TAC measurement if we have enough accelerometer data points\n",
    "                if len(acc_subset) == self.sequence_length:\n",
    "                    x = acc_subset[['x', 'y', 'z']].values\n",
    "                    y = tac_value\n",
    "                    data_list.append((x, y))\n",
    "        \n",
    "        return data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.data[idx]\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "# Usage\n",
    "data_dir = './data'  # Directory containing the CSV files\n",
    "sequence_length = 1000  # 1000 data points before each TAC measurement\n",
    "\n",
    "dataset = TACDataset(data_dir=data_dir, sequence_length=sequence_length)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Example of iterating through the dataloader\n",
    "for batch_x, batch_y in dataloader:\n",
    "    print(batch_x.shape)  # Should be [Batch, 1000, 3]\n",
    "    print(batch_y.shape)  # Should be [Batch]\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_sh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
